{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "36KJcVI-4ueE",
        "outputId": "89be387a-7d95-4887-de1f-af6ab803fc8d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-09e797ed-8bd9-4147-8b56-201a4e0c399c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-09e797ed-8bd9-4147-8b56-201a4e0c399c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUoKrb-R3vzO"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQgirJNd8jD3"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c sheep-classification-challenge-2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pADps3BX8oWT"
      },
      "outputs": [],
      "source": [
        "## Data Loading & Understanding\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "\n",
        "## Data EDA & Preprocessing\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "from skimage import io, color, feature, exposure\n",
        "from skimage.util import img_as_ubyte\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "##  Modelling\n",
        "import timm\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "## Evaluation\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoaR2uWi9WlL"
      },
      "outputs": [],
      "source": [
        "!mkdir -p model\n",
        "!mkdir -p pretrained\n",
        "!mkdir -p dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIpHzw2m9bfR"
      },
      "outputs": [],
      "source": [
        "##################### Saving Prereained Models ######################\n",
        "\n",
        "import torch\n",
        "import timm\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "\n",
        "models_to_save = [\n",
        "    'efficientnet_b0',\n",
        "    'mobilenetv3_large_100',\n",
        "    'resnet18',\n",
        "    'convnext_tiny',\n",
        "    'swin_tiny_patch4_window7_224',\n",
        "    'vit_base_patch16_224',\n",
        "    'convnext_base',\n",
        "    \"densenet121\",\n",
        "    \"efficientnetv2_s\"\n",
        "]\n",
        "\n",
        "for name in models_to_save:\n",
        "    print(f\"ðŸ”¹ Downloading & saving {name}...\")\n",
        "    model = timm.create_model(name, pretrained=True)\n",
        "    torch.save(model.state_dict(), f\"pretrained/{name}_pretrained.pth\")\n",
        "\n",
        "print(\"âœ… All models saved locally!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAkLs_mg9c5h"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3PZ9gy99fFZ"
      },
      "outputs": [],
      "source": [
        "!unzip sheep-classification-challenge-2025 -d dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHqW5RZ8_WT6"
      },
      "outputs": [],
      "source": [
        "################ Load Data\n",
        "MODE = \"folder\"  # \"folder\" atau \"csv\"\n",
        "IMAGE_DIR = \"/content/dataset/Sheep Classification Images/train\"\n",
        "train_csv = \"/content/dataset/Sheep Classification Images/train_labels.csv\"\n",
        "\n",
        "img=\"filename\"\n",
        "label=\"label\"\n",
        "\n",
        "def load_dataset(image_path=\"filename\", labels=\"genus\", data_dir=None, csv_path=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    assert all(c in df.columns for c in [image_path, labels])\n",
        "\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    print(f\"Terdapat {len(df)} data citra dan {df[labels].nunique()} label\")\n",
        "    return df\n",
        "\n",
        "df_train = load_dataset(csv_path=train_csv, labels=\"label\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWrRrduFBLvm"
      },
      "outputs": [],
      "source": [
        "def show_samples(df, data_dir , n_per_class=3, image_paths = \"filename\", labels = \"genus\"):\n",
        "    classes = df[labels].unique()\n",
        "    for cls in classes:\n",
        "        subset = df[df[labels] == cls].sample(min(n_per_class, len(df[df[labels] == cls])))\n",
        "        plt.figure(figsize = (n_per_class * 2, 2))\n",
        "        for i, (_, row) in enumerate (subset.iterrows()):\n",
        "            if data_dir is not None:\n",
        "                img = Image.open(os.path.join(data_dir, row[image_paths]))\n",
        "            else:\n",
        "                img = Image.open(row[image_paths])\n",
        "            plt.subplot(1, n_per_class, i + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.title(cls)\n",
        "        plt.tight_layout\n",
        "        plt.show\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbm2Mo45BqCQ"
      },
      "outputs": [],
      "source": [
        "show_samples(df_train, data_dir = IMAGE_DIR, labels = label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu49tKKXB0pZ"
      },
      "outputs": [],
      "source": [
        "print(df_train.head())\n",
        "print(\"\\n Class Distribution\")\n",
        "print(df_train['label'].value_counts())\n",
        "df_train['label'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHrHAoMWDgcT"
      },
      "outputs": [],
      "source": [
        "width, heights, ratio = [], [], []\n",
        "\n",
        "for path in tqdm(df_train[\"filename\"], desc = \"Analyzing image sizes\"):\n",
        "    try:\n",
        "        with Image.open(os.path.join(IMAGE_DIR, path)) as img:\n",
        "            w, h = img.size\n",
        "            width.append(w)\n",
        "            heights.append(h)\n",
        "            ratio.append(w / h)\n",
        "    except:\n",
        "        print(f\"Image {os.path.join(IMAGE_DIR, path)} cant be opened adding null data\")\n",
        "        width.append(None)\n",
        "        heights.append(None)\n",
        "        ratio.append(None)\n",
        "df_train[\"width\"] = width\n",
        "df_train[\"heights\"] = heights\n",
        "df_train[\"ratio\"] = ratio\n",
        "\n",
        "df_train[['width', 'heights', 'ratio']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0smz40bpcLUI"
      },
      "outputs": [],
      "source": [
        "df_train[['width', 'heights', 'ratio']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cmxRnqMDjFi"
      },
      "outputs": [],
      "source": [
        "def image_stats(df, sample_size = 200, image_col=\"filename\", data_dir = None, per_class = False, label_col = 'genus'):\n",
        "    sample_df = df.sample(min(sample_size, len(df))).reset_index(drop=True)\n",
        "    mean_rgb, std_rgb = [], []\n",
        "    per_class_stats = {}\n",
        "\n",
        "    for _, row in tqdm (sample_df.iterrows(), total = len(sample_df), desc=\"computing RGB stats\"):\n",
        "        path = row[image_col]\n",
        "        label = row[label_col] if label_col in row else \"unknown\"\n",
        "        if data_dir is not None:\n",
        "            path = os.path.join(data_dir, path)\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            continue\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
        "\n",
        "        mean = img.mean(axis=(0, 1))\n",
        "        std = img.std(axis=(0, 1))\n",
        "\n",
        "        mean_rgb.append(mean)\n",
        "        std_rgb.append(std)\n",
        "\n",
        "        if per_class:\n",
        "            if label not in per_class_stats:\n",
        "                per_class_stats[label] = {\"mean\": [], \"std\": []}\n",
        "            per_class_stats[label][\"mean\"].append(mean)\n",
        "            per_class_stats[label][\"std\"].append(std)\n",
        "\n",
        "    mean_rgb = np.mean(mean_rgb, axis = 0)\n",
        "    std_rgb = np.mean(std_rgb, axis = 0)\n",
        "\n",
        "    if per_class:\n",
        "        for cls in per_class_stats:\n",
        "            per_class_stats[cls][\"mean\"] = np.mean(per_class_stats[cls][\"mean\"], axis = 0)\n",
        "            per_class_stats[cls][\"std\"] = np.mean(per_class_stats[cls][\"std\"], axis = 0)\n",
        "        return mean_rgb, std_rgb, per_class_stats\n",
        "    else:\n",
        "        return mean_rgb, std_rgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5Y4ypilDv-n"
      },
      "outputs": [],
      "source": [
        "mean_rgb, std_rgb, per_class_stats = image_stats(df_train, data_dir = IMAGE_DIR, per_class = True, label_col=label)\n",
        "\n",
        "print(f\"Global mean RGB: {mean_rgb}\")\n",
        "print(f\"Global std RGB: {std_rgb}\")\n",
        "for cls, stats in per_class_stats.items():\n",
        "    print(f\"{cls}: Mean {stats[\"mean\"]}, Std {stats[\"std\"]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O5OYz8mDwSt"
      },
      "outputs": [],
      "source": [
        "##### A. Brightness & Kontras###\n",
        "def brightness_contrast(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    brightness = np.mean(gray)\n",
        "    contrast = np.std(gray)\n",
        "    return brightness, contrast\n",
        "\n",
        "######## Blur / Focus Level (Variance of Laplacian)####\n",
        "\n",
        "def blur_score(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "\n",
        "########C. Edge Density (Canny Edge Detector)####\n",
        "\n",
        "def edge_density(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    return np.sum(edges > 0) / edges.size\n",
        "\n",
        "\n",
        "######## D. Entropy (skimage)####\n",
        "from skimage.measure import shannon_entropy\n",
        "def image_entropy(img):\n",
        "    gray = color.rgb2gray(img)\n",
        "    return shannon_entropy(gray)\n",
        "\n",
        "##### E. GLCM (Gray Level Co-occurrence Matrix) Texture Contrast######\n",
        "\n",
        "def glcm_contrast(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    gray = img_as_ubyte(gray)\n",
        "    glcm = graycomatrix(gray, distances=[1], angles=[0], symmetric=True, normed=True)\n",
        "    return graycoprops(glcm, 'contrast')[0,0]\n",
        "\n",
        "\n",
        "######## F. Dominant Color (KMeans)####\n",
        "\n",
        "def dominant_color(img, k=3):\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img_flat = img_rgb.reshape(-1, 3)\n",
        "    km = KMeans(n_clusters=k, n_init=10)\n",
        "    km.fit(img_flat)\n",
        "    return km.cluster_centers_[np.argmax(np.bincount(km.labels_))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkV1Hp5XEdx3"
      },
      "outputs": [],
      "source": [
        "\n",
        "######## Calculate per Image ####\n",
        "features = []\n",
        "for i, row in tqdm(df_train.iterrows(), total=200):\n",
        "    img = cv2.imread(os.path.join(IMAGE_DIR, row['filename']))\n",
        "    if img is None:\n",
        "        continue\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    bright, contrast = brightness_contrast(img)\n",
        "    blur = blur_score(img)\n",
        "    edge = edge_density(img)\n",
        "    entropy = image_entropy(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    glcm_c = glcm_contrast(img)\n",
        "    dom_col = dominant_color(img)\n",
        "\n",
        "    features.append({\n",
        "        'label': row['label'],\n",
        "        'brightness': bright,\n",
        "        'contrast': contrast,\n",
        "        'blur': blur,\n",
        "        'edges': edge,\n",
        "        'entropy': entropy,\n",
        "        'texture_contrast': glcm_c,\n",
        "        'dom_R': dom_col[0],\n",
        "        'dom_G': dom_col[1],\n",
        "        'dom_B': dom_col[2]\n",
        "    })\n",
        "\n",
        "eda_df = pd.DataFrame(features)\n",
        "eda_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llhw5MYwYf6E"
      },
      "outputs": [],
      "source": [
        "eda_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pg3IHUtEe2L"
      },
      "outputs": [],
      "source": [
        "desc = eda_df.groupby('label').describe()\n",
        "desc.columns = ['_'.join(col) for col in desc.columns]\n",
        "desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRqzxg-cGYwC"
      },
      "outputs": [],
      "source": [
        "le_eda = LabelEncoder()\n",
        "eda_df[\"label\"] = le.fit_transform(eda_df[\"label\"])\n",
        "corr = eda_df.corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYdnAL4YNV29"
      },
      "outputs": [],
      "source": [
        "x = eda_df.drop(columns=[label])\n",
        "y = eda_df[label]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': x.columns,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJeodQbxyMmG"
      },
      "outputs": [],
      "source": [
        "le_train = LabelEncoder()\n",
        "df_train[\"label\"] = le_train.fit_transform(df_train[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNDe2N9TVUBZ"
      },
      "outputs": [],
      "source": [
        "class SquarePad:\n",
        "    def __call__(self, image):\n",
        "        w, h = image.size\n",
        "        max_wh = max(w, h)\n",
        "        pad_left = (max_wh - w) // 2\n",
        "        pad_top = (max_wh - h) // 2\n",
        "        pad_right = max_wh - w - pad_left\n",
        "        pad_bottom = max_wh - h - pad_top\n",
        "        return transforms.functional.pad(image, (pad_left, pad_top, pad_right, pad_bottom), 0, 'constant')\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    SquarePad(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    SquarePad(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        "),\n",
        "])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-PneSnjltdg"
      },
      "outputs": [],
      "source": [
        "########## Normal Image Dataset ##########\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, transform, dir_path, special_label=None,  mode=\"train\"):\n",
        "        self.df = df.reset_index()\n",
        "        self.transform = transform\n",
        "        self.dir_path = dir_path\n",
        "        self.special_label = special_label\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df.loc[idx, 'filename']\n",
        "        label = self.df.loc[idx, 'label']\n",
        "        image = Image.open(os.path.join(self.dir_path, img_path)).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opdNoPHvtuJ9"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = train_test_split(df_train, test_size=0.2, random_state=42, stratify =df_train[\"label\"])\n",
        "\n",
        "train_data = ImageDataset(train_data, transforms_train, IMAGE_DIR, mode=\"train\")\n",
        "val_data = ImageDataset(val_data, transforms_val, IMAGE_DIR, mode=\"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiIDMqufu3fR"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jg4x7_LvDtj"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "     def __init__(self, model, device, criterion, optimizer, early_stopping_patience, save_path, wscheduler=None):\n",
        "         self.model = model.to(device)\n",
        "         self.device = device\n",
        "         self.criterion = criterion\n",
        "         self.optimizer = optimizer\n",
        "         self.scheduler =scheduler\n",
        "         self.early_stopping_patience = early_stopping_patience\n",
        "         self.save_path = save_path\n",
        "         self.best_val_loss = 0.0\n",
        "         self.epochs_no_improve = 0\n",
        "\n",
        "     def train_one_epoch(self, train_loader):\n",
        "            self.model.train()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            correct, total = 0,0\n",
        "\n",
        "            for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "\n",
        "            train_acc = 100 * correct / total\n",
        "            train_loss = running_loss / total\n",
        "\n",
        "            return train_loss, train_acc\n",
        "\n",
        "     def validate_per_epoch(self, val_loader):\n",
        "            val_loss, val_correct, val_total = 0.00, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "                    images, labels = images.to(self.device), labels.to(self.device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item() * images.size(0)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (preds == labels).sum().item()\n",
        "\n",
        "            epoch_loss = val_loss/val_total\n",
        "            epoch_acc = 100 * val_correct / val_total\n",
        "\n",
        "            return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "     def fit(self, train_loader, val_loader, num_epochs):\n",
        "            history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                print(f\"epochs {epoch}/{num_epochs}\")\n",
        "                train_loss, train_acc = self.train_one_epoch(train_loader)\n",
        "                val_loss, val_acc = self.validate_per_epoch(val_loader)\n",
        "\n",
        "                history[\"train_loss\"].append(train_loss)\n",
        "                history[\"train_acc\"].append(train_acc)\n",
        "                history[\"val_loss\"].append(val_loss)\n",
        "                history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "                print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\"\n",
        "                     f\"\\nValidation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "                if val_loss < self.best_val_loss:\n",
        "                    self.best_val_loss = val_loss\n",
        "                    self.save = f\"model/model-epoch{epoch}-best.pth\"\n",
        "                    torch.save(self.model.state_dict(), self.save_path)\n",
        "                    self.epochs_no_improve = 0\n",
        "                else:\n",
        "                    self.epochs_no_improve += 1\n",
        "                    if self.early_stopping_patience and self.epochs_no_improve > self.early_stopping_patience:\n",
        "                        self.save_path = \"model/finetuned-model.pth\"\n",
        "                        torch.save(self.model.state_dict(), self.save_path)\n",
        "                        print(f\"Early stopped latest model saved {self.save_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-mLyZd0vPtE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "class HybridFineTune(nn.Module):\n",
        "    def __init__(self, num_classes, convnext_weight_path=None, freeze_backbone=True, dropout=0.3):\n",
        "        super(HybridFineTune, self).__init__()\n",
        "\n",
        "        # === EfficientNet backbone (fine-grained features) ===\n",
        "        self.eff = timm.create_model('efficientnetv2_s', pretrained=False, num_classes=0)\n",
        "        eff_features = self.eff.num_features\n",
        "\n",
        "        # === ConvNeXt backbone (general features) ===\n",
        "        self.convnext = timm.create_model('convnext_tiny', pretrained=False, num_classes=0)\n",
        "        if convnext_weight_path is not None:\n",
        "            print(f\"Loading ConvNeXt weights from {convnext_weight_path} ...\")\n",
        "            state_dict = torch.load(convnext_weight_path, map_location='cpu')\n",
        "            self.convnext.load_state_dict(state_dict, strict=False)\n",
        "        conv_features = self.convnext.num_features\n",
        "\n",
        "        # === Freeze backbone (optional) ===\n",
        "        if freeze_backbone:\n",
        "            for p in self.eff.parameters():\n",
        "                p.requires_grad = False\n",
        "            for p in self.convnext.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # === Fusion layer ===\n",
        "        total_features = eff_features + conv_features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(total_features, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        eff_out = self.eff(x)\n",
        "        conv_out = self.convnext(x)\n",
        "\n",
        "        # Concatenate kedua feature\n",
        "        combined = torch.cat((eff_out, conv_out), dim=1)\n",
        "        out = self.classifier(combined)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky6gjPjavk12"
      },
      "outputs": [],
      "source": [
        "model = HybridFineTune(convnext_weight_path= \"/content/pretrained/convnext_tiny_pretrained.pth\", num_classes = int(df_train[\"label\"].nunique()), freeze_backbone= False)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdvaUHvnv7_a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    device = device,\n",
        "    criterion = criterion,\n",
        "    optimizer = optimizer,\n",
        "    early_stopping_patience = 10,\n",
        "    save_path = \"model/hybridfinetuned.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkDBteOwv_G4"
      },
      "outputs": [],
      "source": [
        "history = trainer.fit(train_loader, val_loader, num_epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jAdq0Hd0a-j"
      },
      "outputs": [],
      "source": [
        "model = HybridFineTune(convnext_weight_path= \"/content/pretrained/convnext_tiny_pretrained.pth\", num_classes = int(df_train[\"label\"].nunique()), freeze_backbone= False)\n",
        "model.load_state_dict(torch.load(\"/content/model/finetuned-model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd2olQev0mti"
      },
      "outputs": [],
      "source": [
        "pred_val = []\n",
        "label_val = []\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "val_loss, val_correct, val_total = 0.00, 0, 0\n",
        "with torch.no_grad():\n",
        "      for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        val_total += labels.size(0)\n",
        "        val_correct += (preds == labels).sum().item()\n",
        "\n",
        "        label_val.extend(labels.cpu().numpy())\n",
        "        pred_val.extend(preds.cpu().numpy())\n",
        "\n",
        "epoch_loss = val_loss/val_total\n",
        "epoch_acc = 100 * val_correct / val_total\n",
        "\n",
        "print(classification_report(label_val, pred_val))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHGGeXf20Jc_"
      },
      "outputs": [],
      "source": [
        "image_paths = \"/content/dataset/Sheep Classification Images/test\"\n",
        "\n",
        "\n",
        "images = []\n",
        "for image in os.listdir(image_paths):\n",
        "    files = os.path.join(image_paths, image)\n",
        "    images.append(image)\n",
        "\n",
        "df_test = pd.DataFrame({\"filename\": images})\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDukkA3hxpe4"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, transform, dir_path):\n",
        "        self.df = df.reset_index()\n",
        "        self.transform = transform\n",
        "        self.dir_path = dir_path\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df.loc[idx, 'filename']\n",
        "        image = Image.open(os.path.join(self.dir_path, img_path)).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88cN1tWqytti"
      },
      "outputs": [],
      "source": [
        "test_data = TestDataset(df_test, transform = transforms_val, dir_path = image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHPoYEuTyuPp"
      },
      "outputs": [],
      "source": [
        "test_loader = DataLoader(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKJtB71Z4qKk"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5aNcsrbyx9H"
      },
      "outputs": [],
      "source": [
        "pred = []\n",
        "\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "    for images in tqdm(test_loader, desc=\"Testing\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        pred.append(int(preds.item()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hySNyioY3xrW"
      },
      "outputs": [],
      "source": [
        "df_test[\"label\"] = le_train.inverse_transform(pred)\n",
        "\n",
        "\n",
        "df_test[[\"filename\", \"label\"]].to_csv(\"result_pred.csv\",  index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYx05IPm6Jgv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_-MymQu5Eq_"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c sheep-classification-challenge-2025 -f result_pred.csv -m \"eddicientnetxconvnexttint\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXbI5LY35puH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
